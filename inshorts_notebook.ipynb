{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inshorts_notebook.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNcz62G7-c0B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BASE_PATH = \"~/drive/colaboratory/Hacktuit/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejGWvS3CZ8O0",
        "colab_type": "code",
        "outputId": "bd2998e8-9f54-44b3-c1f7-48de12d55fde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "!wget -nc http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
        "!unzip glove.840B.300d.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-07-22 17:18:33--  http://nlp.stanford.edu/data/glove.840B.300d.zip\r\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.840B.300d.zip [following]\n",
            "--2018-07-22 17:18:34--  https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2176768927 (2.0G) [application/zip]\n",
            "Saving to: ‘glove.840B.300d.zip’\n",
            "\n",
            "glove.840B.300d.zip  18%[==>                 ] 387.57M  9.33MB/s    eta 2m 39s "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "glove.840B.300d.zip 100%[===================>]   2.03G  17.8MB/s    in 3m 21s  \n",
            "\n",
            "2018-07-22 17:21:56 (10.3 MB/s) - ‘glove.840B.300d.zip’ saved [2176768927/2176768927]\n",
            "\n",
            "Archive:  glove.840B.300d.zip\n",
            "  inflating: glove.840B.300d.txt     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLMmeEJDXAbD",
        "colab_type": "code",
        "outputId": "f3a00e52-a7f2-449a-bf26-3829e9dab60c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1176
        }
      },
      "source": [
        "!pip install nltk\n",
        "!pip install tqdm\n",
        "!pip install keras_tqdm\n",
        "!pip install ipywidgets"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\r\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\n",
            "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
            "Collecting tqdm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/24/6ab1df969db228aed36a648a8959d1027099ce45fad67532b9673d533318/tqdm-4.23.4-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 4.9MB/s \n",
            "\u001b[?25hInstalling collected packages: tqdm\n",
            "Successfully installed tqdm-4.23.4\n",
            "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
            "Collecting keras_tqdm\n",
            "  Downloading https://files.pythonhosted.org/packages/16/5c/ac63c65b79a895b8994474de2ad4d5b66ac0796b8903d60cfea3f8308d5c/keras_tqdm-2.0.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from keras_tqdm) (4.23.4)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras_tqdm) (2.1.6)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_tqdm) (0.19.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras_tqdm) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_tqdm) (1.14.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras_tqdm) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_tqdm) (1.11.0)\n",
            "Installing collected packages: keras-tqdm\n",
            "Successfully installed keras-tqdm-2.0.1\n",
            "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
            "Collecting ipywidgets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/5d/868df21e3b004a5a61294cab70e1f6f44986933eb3aa9c396dfd5112acb2/ipywidgets-7.3.0-py2.py3-none-any.whl (109kB)\n",
            "\u001b[K    100% |████████████████████████████████| 112kB 6.6MB/s \n",
            "\u001b[?25hCollecting widgetsnbextension~=3.3.0 (from ipywidgets)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/43/f6ff09448f7b961e102fd75b7e46a5d44b68b9746bb1ab5c4be64c3e236d/widgetsnbextension-3.3.0-py2.py3-none-any.whl (2.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.2MB 12.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets) (5.5.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets) (4.4.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets) (4.6.1)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets) (4.3.2)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from widgetsnbextension~=3.3.0->ipywidgets) (5.2.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (39.1.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (4.3.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (1.0.15)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.7.4)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (2.1.3)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (4.6.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.8.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets) (2.6.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets) (4.4.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.1->ipywidgets) (4.5.3)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.2.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.3.1->ipywidgets) (1.11.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.3.0->ipywidgets) (5.3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.3.0->ipywidgets) (2.10)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.3.0->ipywidgets) (0.8.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.6.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (16.0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.5.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.3.0->ipywidgets) (0.3.1)\n",
            "Requirement already satisfied: mistune>=0.7.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.3.0->ipywidgets) (0.8.3)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.3.0->ipywidgets) (0.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.3.0->ipywidgets) (2.1.3)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.3.0->ipywidgets) (1.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.3.0->ipywidgets) (1.0)\n",
            "Requirement already satisfied: html5lib!=1.0b1,!=1.0b2,!=1.0b3,!=1.0b4,!=1.0b5,!=1.0b6,!=1.0b7,!=1.0b8,>=0.99999999pre in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.3.0->ipywidgets) (1.0.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from html5lib!=1.0b1,!=1.0b2,!=1.0b3,!=1.0b4,!=1.0b5,!=1.0b6,!=1.0b7,!=1.0b8,>=0.99999999pre->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.3.0->ipywidgets) (0.5.1)\n",
            "Installing collected packages: widgetsnbextension, ipywidgets\n",
            "Successfully installed ipywidgets-7.3.0 widgetsnbextension-3.3.0\n",
            "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYmdlx5vcHLj",
        "colab_type": "code",
        "outputId": "126d54c4-e8a5-437f-afca-e2f9304f5185",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import backend as K\n",
        "from keras_tqdm import TQDMNotebookCallback\n",
        "from IPython import display"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Da17xRpehQJ",
        "colab_type": "code",
        "outputId": "44cd0d54-f708-41ee-d334-5341162a036f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# load the GloVe vectors in a dictionary:\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open('glove.840B.300d.txt')\n",
        "for line in tqdm(f):\n",
        "    values = line.split(' ')\n",
        "    word = values[0]\n",
        "    try:\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "    except:\n",
        "      print (\"Ignoring word: \", word)\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2196017it [03:19, 10985.12it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 2196016 word vectors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdZO0TbtW5_S",
        "colab_type": "code",
        "outputId": "ba9d496d-9617-49f9-da50-34d73f34e631",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "train_df = pd.read_csv(BASE_PATH + 'inshorts_train.csv')\n",
        "train_df['Category'] = train_df['Category'].map({\n",
        "    \"business\" : 0,\n",
        "    \"entertainment\" : 1,\n",
        "    \"national\" : 2,\n",
        "    \"politics\" : 3,\n",
        "    \"science\" : 4,\n",
        "    \"sports\" : 5,\n",
        "    \"technology\": 6,\n",
        "    \"world\" : 7\n",
        "})\n",
        "test_df = pd.read_csv(BASE_PATH + 'inshorts_test.csv')\n",
        "concat_df = test_df.append(train_df)\n",
        "print (train_df.shape)\n",
        "print (test_df.shape)\n",
        "print (concat_df.shape)\n",
        "\n",
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64253, 5)\n",
            "(16064, 4)\n",
            "(80317, 5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Date</th>\n",
              "      <th>Title</th>\n",
              "      <th>Article</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>528</td>\n",
              "      <td>05 Apr 2018,Thursday</td>\n",
              "      <td>4 bank employees booked in alleged scam of â‚¹...</td>\n",
              "      <td>The Rajasthan police has registered a case aga...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>51112</td>\n",
              "      <td>08 Dec 2017,Friday</td>\n",
              "      <td>Gayle hits 45-ball ton, becomes first to reach...</td>\n",
              "      <td>Windies' Chris Gayle smashed a 45-ball hundred...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>73666</td>\n",
              "      <td>28 Oct 2017,Saturday</td>\n",
              "      <td>Taiwan holds Asia's largest gay pride parade</td>\n",
              "      <td>Taiwan on Saturday held Asia's largest gay pri...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13719</td>\n",
              "      <td>07 Dec 2017,Thursday</td>\n",
              "      <td>Arjun slams reports of man assaulting him on f...</td>\n",
              "      <td>Arjun Kapoor has slammed reports of a man assa...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17356</td>\n",
              "      <td>30 Jun 2017,Friday</td>\n",
              "      <td>Maybe, I'm the most spoilt superstar but Iove ...</td>\n",
              "      <td>Speaking on his stardom, actor Shah Rukh Khan ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      ID                  Date  \\\n",
              "0    528  05 Apr 2018,Thursday   \n",
              "1  51112    08 Dec 2017,Friday   \n",
              "2  73666  28 Oct 2017,Saturday   \n",
              "3  13719  07 Dec 2017,Thursday   \n",
              "4  17356    30 Jun 2017,Friday   \n",
              "\n",
              "                                               Title  \\\n",
              "0  4 bank employees booked in alleged scam of â‚¹...   \n",
              "1  Gayle hits 45-ball ton, becomes first to reach...   \n",
              "2       Taiwan holds Asia's largest gay pride parade   \n",
              "3  Arjun slams reports of man assaulting him on f...   \n",
              "4  Maybe, I'm the most spoilt superstar but Iove ...   \n",
              "\n",
              "                                             Article  Category  \n",
              "0  The Rajasthan police has registered a case aga...         0  \n",
              "1  Windies' Chris Gayle smashed a 45-ball hundred...         5  \n",
              "2  Taiwan on Saturday held Asia's largest gay pri...         7  \n",
              "3  Arjun Kapoor has slammed reports of a man assa...         1  \n",
              "4  Speaking on his stardom, actor Shah Rukh Khan ...         1  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zOVAD3amQII",
        "colab_type": "code",
        "outputId": "745158bc-ec8d-4790-aeb7-0fee8f41ce7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences \n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 200\n",
        "\n",
        "vocabulary = dict()\n",
        "inverse_vocabulary = ['PADDING']\n",
        "\n",
        "def generate_sequence(text_list):\n",
        "    tknzr = TweetTokenizer(reduce_len=True)\n",
        "  \n",
        "    sequences = []\n",
        "    \n",
        "    for text in text_list:\n",
        "        #text = text.split()\n",
        "        text = tknzr.tokenize(text.lower())\n",
        "        text_sequence = []\n",
        "        for word in text:\n",
        "            if word not in vocabulary:\n",
        "                vocabulary[word] = len(inverse_vocabulary)\n",
        "                text_sequence.append(len(inverse_vocabulary))\n",
        "                inverse_vocabulary.append(word)\n",
        "            else:\n",
        "                text_sequence.append(vocabulary[word])\n",
        "        sequences.append(text_sequence)\n",
        "    print(\"%d unique tokens in the vocabulary\" %len(vocabulary))\n",
        "\n",
        "    return sequences\n",
        "\n",
        "concat_seq = generate_sequence(concat_df.Article.values.tolist())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "97364 unique tokens in the vocabulary\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g254MwnEKMM9",
        "colab_type": "code",
        "outputId": "2de22113-178e-4c75-e801-70c6d137e5af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_seq = generate_sequence(train_df.Article.values.tolist())\n",
        "bodies_seq = pad_sequences(train_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "labels = train_df.Category.values.tolist()\n",
        "\n",
        "data = {\n",
        "    'train': {},\n",
        "    'val': {},\n",
        "    'test': {},\n",
        "}\n",
        "\n",
        "(data['train']['text'],\n",
        " data['val']['text'],\n",
        " data['train']['y'],\n",
        " data['val']['y']\n",
        ") = train_test_split(bodies_seq, labels, test_size=0.3, random_state=42)\n",
        "\n",
        "(data['val']['text'],\n",
        " data['test']['text'],\n",
        " data['val']['y'],\n",
        " data['test']['y']\n",
        ") = train_test_split(data['val']['text'], data['val']['y'], test_size=0.1, random_state=42)\n",
        "\n",
        "print('Train: {} examples'.format(len(data['train']['y'])))\n",
        "print('Val: {} examples'.format(len(data['val']['y'])))\n",
        "print('Test: {} examples'.format(len(data['test']['y'])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "97364 unique tokens in the vocabulary\n",
            "Train: 44977 examples\n",
            "Val: 17348 examples\n",
            "Test: 1928 examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkWv-JVo1vx",
        "colab_type": "code",
        "outputId": "fd87c5a5-88b8-4839-a4e6-eb05e9fa5813",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "EMBEDDING_DIM = 300\n",
        "\n",
        "embedding_matrix = np.zeros((len(vocabulary) + 1, EMBEDDING_DIM)) \n",
        "for word, i in vocabulary.items():\n",
        "    if word in embeddings_index.keys():\n",
        "        embedding_matrix[i] = embeddings_index[word]\n",
        "\n",
        "print (embedding_matrix.shape)\n",
        "\n",
        "embedding_layer = layers.Embedding(\n",
        "    len(vocabulary) + 1,\n",
        "    EMBEDDING_DIM,\n",
        "    weights=[embedding_matrix],\n",
        "    input_length=MAX_SEQUENCE_LENGTH,\n",
        "    name='Embedding',\n",
        "    trainable=False\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(97365, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nkpJw0KsY0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import concatenate,Input,Dense, GlobalAveragePooling1D, Embedding, Dropout\n",
        "\n",
        "class CnnModel(keras.models.Model):\n",
        "    \n",
        "    def __init__(self, filter_sizes, weights_matrix=embedding_matrix):\n",
        "        replicate_count = 10\n",
        "        projection_dropout = 0.5\n",
        "\n",
        "        input_layer = layers.Input(\n",
        "            shape=(MAX_SEQUENCE_LENGTH,),\n",
        "            name='Input'\n",
        "        )\n",
        "        \n",
        "        eb_layer = embedding_layer(input_layer)\n",
        "                \n",
        "        conv_layers = []\n",
        "        for filter_size in filter_sizes:\n",
        "            for _r in range(replicate_count):\n",
        "                str_id = '_' + str(filter_size) + '_' + str(_r)\n",
        "                conv = layers.Conv1D(\n",
        "                    EMBEDDING_DIM, \n",
        "                    filter_size, \n",
        "                    activation='relu',\n",
        "                    name='Conv1D-' + str_id\n",
        "                )(eb_layer)\n",
        "                max_pool = layers.GlobalMaxPooling1D(\n",
        "                    name='MaxPool' + str_id\n",
        "                )(conv)\n",
        "                dropout  = layers.Dropout(\n",
        "                    rate=projection_dropout\n",
        "                )(max_pool)\n",
        "                conv_layers.append(dropout)\n",
        "\n",
        "        concatenated_layer = concatenate(\n",
        "            conv_layers,\n",
        "            name='ConcatenatedRegions'\n",
        "        )\n",
        "        \n",
        "        fc_layer = layers.Dense(\n",
        "            units=10,\n",
        "            name='FullyConnected'\n",
        "        )(concatenated_layer)\n",
        "        \n",
        "        dropout_layer = layers.Dropout(\n",
        "            rate=projection_dropout\n",
        "        )(fc_layer)\n",
        "             \n",
        "        predictions = layers.Dense(\n",
        "            8,\n",
        "            activation='softmax',\n",
        "            name='Output'\n",
        "        )(dropout_layer)\n",
        "        \n",
        "        super().__init__(inputs=[input_layer], outputs=predictions)\n",
        "          \n",
        "    def compile(self):\n",
        "        return super().compile(\n",
        "            optimizer=keras.optimizers.Adam(lr=0.001),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdxtwyXNpikn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LstmModel(keras.models.Model):\n",
        "    \n",
        "    def __init__(self, weights_matrix=embedding_matrix):\n",
        "        projection_dropout = 0.5\n",
        "\n",
        "        input_layer = layers.Input(\n",
        "            shape=(MAX_SEQUENCE_LENGTH,),\n",
        "            name='Input'\n",
        "        )\n",
        "        \n",
        "        eb_layer = embedding_layer(input_layer)\n",
        "\n",
        "        # spatial_dropout_layer = layers.SpatialDropout1D(0.4)(eb_layer)\n",
        "        \n",
        "        bilstm_layer = layers.Bidirectional(\n",
        "            layers.LSTM(\n",
        "                units=200,\n",
        "                dropout=0.5, \n",
        "                recurrent_dropout=0.3,\n",
        "            )\n",
        "        )(eb_layer)\n",
        "        \n",
        "        fc_layer = layers.Dense(\n",
        "            units=10,\n",
        "            activation='relu',\n",
        "            name='FullyConnected'\n",
        "        )(bilstm_layer)\n",
        "        \n",
        "        dropout_layer = layers.Dropout(\n",
        "            rate=projection_dropout\n",
        "        )(fc_layer)\n",
        "        \n",
        "        predictions = layers.Dense(\n",
        "            8,\n",
        "            activation='softmax',\n",
        "            name='Output'\n",
        "        )(dropout_layer)\n",
        "        \n",
        "        super().__init__(inputs=[input_layer], outputs=predictions)\n",
        "        \n",
        "    def compile(self):\n",
        "        return super().compile(\n",
        "            optimizer=keras.optimizers.Adam(lr=0.001),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IWX_0EGUZmk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GruModel(keras.models.Model):\n",
        "    \n",
        "    def __init__(self, weights_matrix=embedding_matrix):\n",
        "        projection_dropout = 0.5\n",
        "\n",
        "        input_layer = layers.Input(\n",
        "            shape=(MAX_SEQUENCE_LENGTH,),\n",
        "            name='Input'\n",
        "        )\n",
        "        \n",
        "        eb_layer = embedding_layer(input_layer)\n",
        "\n",
        "        # spatial_dropout_layer = layers.SpatialDropout1D(0.4)(eb_layer)\n",
        "        \n",
        "        bilstm_layer = layers.GRU(\n",
        "            units=200,\n",
        "            dropout=0.5, \n",
        "            recurrent_dropout=0.3\n",
        "        )(eb_layer)\n",
        "        \n",
        "        fc_layer = layers.Dense(\n",
        "            units=50,\n",
        "            activation='relu',\n",
        "            name='FullyConnected'\n",
        "        )(bilstm_layer)\n",
        "        \n",
        "        dropout_layer = layers.Dropout(\n",
        "            rate=projection_dropout\n",
        "        )(fc_layer)\n",
        "        \n",
        "        predictions = layers.Dense(\n",
        "            8,\n",
        "            activation='softmax',\n",
        "            name='Output'\n",
        "        )(dropout_layer)\n",
        "        \n",
        "        super().__init__(inputs=[input_layer], outputs=predictions)\n",
        "        \n",
        "    def compile(self):\n",
        "        return super().compile(\n",
        "            optimizer=keras.optimizers.Adam(lr=0.001),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-syZec8K20c",
        "colab_type": "code",
        "outputId": "d5636acd-85d8-4274-8af1-43aaab7235e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "!mkdir models-10\n",
        "\n",
        "\n",
        "\n",
        "def fit_model(model, epochs, batch_size, prefix=\"\"):\n",
        "    filepath = BASE_PATH[2:] + \"models-10/\" + prefix + \"-{epoch:02d}.hdf5\"\n",
        "    encoder = LabelBinarizer()\n",
        "\n",
        "    history = model.fit(\n",
        "        [data['train']['text']],\n",
        "        keras.utils.to_categorical(data['train']['y'], 8),\n",
        "        epochs=epochs,\n",
        "        verbose=1,\n",
        "        callbacks=[\n",
        "            ModelCheckpoint(\n",
        "                filepath, \n",
        "                verbose=1,\n",
        "                save_best_only=True\n",
        "            )\n",
        "        ],\n",
        "        validation_data=(\n",
        "            [data['val']['text']],\n",
        "            keras.utils.to_categorical(data['val']['y'], 8),\n",
        "        ),\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "    return history"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘models-10’: File exists\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wfe6p5rLGo_",
        "colab_type": "code",
        "outputId": "467c28e3-55e5-4b4f-e8c5-7a980d94a95c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "def generate_models():\n",
        "    model = CnnModel([1, 2, 3])\n",
        "    model.compile()\n",
        "    history = fit_model(model, 8, 512, 'CNN-123')\n",
        "\n",
        "    model = CnnModel([3, 4, 5])\n",
        "    model.compile()\n",
        "    history = fit_model(model, 30, 512, 'CNN-345')\n",
        "\n",
        "    model = LstmModel()\n",
        "    model.compile()\n",
        "    history = fit_model(model, 30, 512, 'BLSTM')\n",
        "    \n",
        "    model = GruModel()\n",
        "    model.compile()\n",
        "    history = fit_model(model, 50, 1024, 'GRU')\n",
        "generate_models()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 44977 samples, validate on 17348 samples\n",
            "Epoch 1/8\n",
            "44977/44977 [==============================] - 268s 6ms/step - loss: 2.0610 - acc: 0.5144 - val_loss: 0.6212 - val_acc: 0.8168\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.62118, saving model to drive/colaboratory/Hacktuit/models-10/CNN-123-01.hdf5\n",
            "Epoch 2/8\n",
            "44977/44977 [==============================] - 256s 6ms/step - loss: 1.1142 - acc: 0.6479 - val_loss: 0.5733 - val_acc: 0.8209\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.62118 to 0.57332, saving model to drive/colaboratory/Hacktuit/models-10/CNN-123-02.hdf5\n",
            "Epoch 3/8\n",
            "37888/44977 [========================>.....] - ETA: 34s - loss: 0.9727 - acc: 0.6735"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "44977/44977 [==============================] - 257s 6ms/step - loss: 0.9619 - acc: 0.6768 - val_loss: 0.5523 - val_acc: 0.8303\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.57332 to 0.55226, saving model to drive/colaboratory/Hacktuit/models-10/CNN-123-03.hdf5\n",
            "Epoch 4/8\n",
            "44977/44977 [==============================] - 257s 6ms/step - loss: 0.8485 - acc: 0.7106 - val_loss: 0.5864 - val_acc: 0.8157\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.55226\n",
            "Epoch 5/8\n",
            "44977/44977 [==============================] - 256s 6ms/step - loss: 0.7863 - acc: 0.7264 - val_loss: 0.5420 - val_acc: 0.8110\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.55226 to 0.54198, saving model to drive/colaboratory/Hacktuit/models-10/CNN-123-05.hdf5\n",
            "Epoch 6/8\n",
            "11264/44977 [======>.......................] - ETA: 2:46 - loss: 0.7256 - acc: 0.7433"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "44977/44977 [==============================] - 257s 6ms/step - loss: 0.7025 - acc: 0.7503 - val_loss: 0.5201 - val_acc: 0.8203\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.54198 to 0.52010, saving model to drive/colaboratory/Hacktuit/models-10/CNN-123-06.hdf5\n",
            "Epoch 7/8\n",
            "44977/44977 [==============================] - 257s 6ms/step - loss: 0.6563 - acc: 0.7641 - val_loss: 0.5367 - val_acc: 0.8210\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.52010\n",
            "Epoch 8/8\n",
            "44977/44977 [==============================] - 257s 6ms/step - loss: 0.6134 - acc: 0.7806 - val_loss: 0.5394 - val_acc: 0.8219\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.52010\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeWZFHleLLqd",
        "colab_type": "code",
        "outputId": "931b89ab-b7dd-447b-d717-dbe0c1eca93a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "from os import listdir\n",
        "import random, re\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "def shouldProcess(s):\n",
        "    if 'CNN-' not in s and 'LSTM-' not in s and 'GRU' not in s: return False\n",
        "\n",
        "    epoch = int(re.findall('-([0-9]*).hdf5', s)[0])\n",
        "    if 'CNN-345' in s and epoch > 3: return True\n",
        "    if 'LSTM-' in s and epoch > 14: return True\n",
        "    if 'BLSTM-' in s and epoch > 14: return True\n",
        "    if 'GRU-' in s and epoch > 15: return True\n",
        "    return False\n",
        "\n",
        "model_files = [ str(BASE_PATH[2:] + 'models-10/' + x) for x in listdir(BASE_PATH[2:] + 'models-10/') if shouldProcess(x)]\n",
        "\n",
        "print (model_files)\n",
        "\n",
        "random.seed(42)\n",
        "random.shuffle(model_files)\n",
        "\n",
        "rand_models_files = model_files[:30]\n",
        "\n",
        "models = []\n",
        "for file in rand_models_files:\n",
        "    print (file)\n",
        "    m = None\n",
        "    if 'CNN-123' in file:\n",
        "        m = CnnModel([1, 2, 3])\n",
        "    elif 'CNN-345' in file:\n",
        "        m = CnnModel([3, 4, 5])\n",
        "    elif 'LSTM-' in file:\n",
        "        m = LstmModel()\n",
        "    elif 'GRU-' in file:\n",
        "        m = GruModel()\n",
        "    m.load_weights(file)\n",
        "    models.append((file, m))\n",
        "\n",
        "print (models)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['drive/colaboratory/Hacktuit/models-10/GRU-27.hdf5', 'drive/colaboratory/Hacktuit/models-10/BLSTM-26.hdf5', 'drive/colaboratory/Hacktuit/models-10/CNN-123-06.hdf5']\n",
            "drive/colaboratory/Hacktuit/models-10/BLSTM-26.hdf5\n",
            "drive/colaboratory/Hacktuit/models-10/GRU-27.hdf5\n",
            "drive/colaboratory/Hacktuit/models-10/CNN-123-06.hdf5\n",
            "[('drive/colaboratory/Hacktuit/models-10/BLSTM-26.hdf5', <__main__.LstmModel object at 0x7f513dba6e48>), ('drive/colaboratory/Hacktuit/models-10/GRU-27.hdf5', <__main__.GruModel object at 0x7f51313d1860>), ('drive/colaboratory/Hacktuit/models-10/CNN-123-06.hdf5', <__main__.CnnModel object at 0x7f512a090550>)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igzsc2d8_kpt",
        "colab_type": "code",
        "outputId": "06642ddf-36a6-4a1d-c19c-f47b5eaeec27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "from keras.models import Model\n",
        "\n",
        "def ensembleModels(models, model_input):\n",
        "    # collect outputs of models in a list\n",
        "    yModels=[model(model_input) for model in models] \n",
        "    # averaging outputs\n",
        "    yAvg=layers.average(yModels) \n",
        "    # build model from same input and avg output\n",
        "    modelEns = Model(inputs=model_input, outputs=yAvg,    name='ensemble')  \n",
        "   \n",
        "    return modelEns\n",
        "\n",
        "model_input = Input(shape=(MAX_SEQUENCE_LENGTH,)) # c*h*w\n",
        "modelEns = ensembleModels([m[1] for m in models], model_input)\n",
        "modelEns.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 200)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstmmodel_20 (LstmModel)        (None, 8)            30015198    input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "grumodel_14 (GruModel)          (None, 8)            29520558    input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "cnnmodel_33 (CnnModel)          (None, 8)            34708598    input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "average_3 (Average)             (None, 8)            0           lstmmodel_20[1][0]               \n",
            "                                                                 grumodel_14[1][0]                \n",
            "                                                                 cnnmodel_33[1][0]                \n",
            "==================================================================================================\n",
            "Total params: 35,825,354\n",
            "Trainable params: 6,615,854\n",
            "Non-trainable params: 29,209,500\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IVQZrCNbLU4",
        "colab_type": "code",
        "outputId": "130fce23-f633-4fae-f0bb-e4200c6a7d52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "\n",
        "# h = np.sum([m[1].predict([data['test']['text']]) for m in models], 0)\n",
        "\n",
        "h = modelEns.predict([data['val']['text']])\n",
        "\n",
        "print (h.shape)\n",
        "\n",
        "predict = np.asarray([np.argmax(i) for i in h])\n",
        "gold = np.asarray(data['val']['y'])\n",
        "resultf1 = accuracy_score(gold, predict)\n",
        "print('Accuracy score on {} test samples: {:.6}'.format(\n",
        "    len(predict), resultf1\n",
        "))\n",
        "\n",
        "print (metrics.confusion_matrix(gold, predict))\n",
        "print (metrics.classification_report(gold, predict))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(17348, 8)\n",
            "Accuracy score on 17348 test samples: 0.843728\n",
            "[[1912   19  108   28   10   11  179   95]\n",
            " [  13 2132   20   53    1   57   28   33]\n",
            " [ 170   87 1266  542   13   25   26  109]\n",
            " [  15   31   94 2129    0    6    2    7]\n",
            " [   5    5    9    1 1101    2   36   32]\n",
            " [   2   38    4    3    5 2234    9   16]\n",
            " [ 185   21   22    0  109   14 1870   82]\n",
            " [  70   32   68    7   47   24   81 1993]]\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "          0       0.81      0.81      0.81      2362\n",
            "          1       0.90      0.91      0.91      2337\n",
            "          2       0.80      0.57      0.66      2238\n",
            "          3       0.77      0.93      0.84      2284\n",
            "          4       0.86      0.92      0.89      1191\n",
            "          5       0.94      0.97      0.95      2311\n",
            "          6       0.84      0.81      0.82      2303\n",
            "          7       0.84      0.86      0.85      2322\n",
            "\n",
            "avg / total       0.84      0.84      0.84     17348\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si9Yt4GpHTIE",
        "colab_type": "code",
        "outputId": "17fe1f11-86f1-4a6c-a7ed-9e4ebda4ea6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Generate labels for challenge set\n",
        "test_seq = generate_sequence(test_df.Article.values.tolist())\n",
        "test_seq = pad_sequences(test_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "print (len(test_seq))\n",
        "\n",
        "# test_h = np.sum([m[1].predict([test_seq]) for m in models], 0)\n",
        "test_h = modelEns.predict([test_seq])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "97364 unique tokens in the vocabulary\n",
            "16064\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpfzchyiZaW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outfile = open('test_out.csv', 'w')\n",
        "outfile.write(\"ID,business,entertainment,national,politics,science,sports,technology,world\\n\")\n",
        "for id, col in zip(test_df.ID.values.tolist(), test_h):\n",
        "  #  print (str(id) + ',' + ','.join([format(e, 'f') for e in col]))\n",
        "    outfile.write(str(id) + ',' + ','.join([format(e, 'f') for e in col]) + '\\n')\n",
        "outfile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyW0uLZcdRV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('test_out.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}